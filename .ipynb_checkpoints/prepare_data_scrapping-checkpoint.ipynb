{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from os import makedirs\n",
    "from os.path import exists\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from time import sleep\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gazeta Wyborcza i Gazeta.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'duda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dir = 'data/urls/wyborcza_gazeta/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10001):\n",
    "    txt_html = requests\\\n",
    "        .get(\"http://szukaj.gazeta.pl/wyszukaj/artykul?&query=\"+keyword+\"&sortMode=SCORE&pageNumber=\"+str(i))\\\n",
    "        .text\n",
    "    soup = BeautifulSoup(txt_html)\n",
    "    \n",
    "    elements = soup.find_all('section', 'elem')\n",
    "    if len(elements) != 0:\n",
    "        for elem in elements:\n",
    "            a = elem.header.h3.a\n",
    "            if a:\n",
    "                urls.append(a['href'])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list(set(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10250"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(desired_dir):\n",
    "    if not exists(desired_dir):\n",
    "        makedirs(desired_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_dir(url_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(url_dir+keyword, 'a') as f:\n",
    "    f.write('\\n'.join(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://szukaj.gazeta.pl/wyszukaj/artykul?query=duda&dxx=126213\")\n",
    "sleep(5)\n",
    "try:\n",
    "    driver\\\n",
    "        .find_element_by_xpath('//*[@id=\"rodoNotificationWrapper\"]/div[2]/div/div[3]/button')\\\n",
    "        .click()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(url_dir+keyword, 'r') as f:\n",
    "    urls = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url):\n",
    "    if 'wyborcza.pl' in url:\n",
    "        title = driver.find_element_by_class_name('art-title').text\n",
    "        lead = driver.find_element_by_class_name('article-lead').text\n",
    "        try:\n",
    "            content = driver.find_element_by_id('artykul').text\n",
    "        except:\n",
    "            content = driver.find_element_by_class_name('art_content').text\n",
    "        img_desc =  \"-@@@-\".join([x.text for x in driver.find_elements_by_class_name('article-image-desc')])\n",
    "        comments = \"-@@@-\".join([x.text for x in driver.find_elements_by_class_name('cBody') if x.text != ''])\n",
    "        return title, lead, content, img_desc, comments\n",
    "    elif 'gazeta.pl' in url:\n",
    "        title = driver.find_element_by_id('article_title').text\n",
    "        lead = driver.find_element_by_id('gazeta_article_lead').text\n",
    "        content = driver.find_element_by_class_name('art_content').text\n",
    "        img_desc =  \"-@@@-\".join([x.text for x in driver.find_elements_by_class_name('desc')])\n",
    "        comments = \"-@@@-\".join([x.find_elements_by_tag_name('p')[3].text for x in driver.find_elements_by_class_name('comment-body') if x.text != ''])\n",
    "        return title, lead, content, img_desc, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'https://wyborcza.pl/1,155290,17928296,Duda___zwiazkowcy___katastrofa_gornictwa.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(test)\n",
    "# get_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_urls = []\n",
    "wyborcza_content = []\n",
    "gazeta_content = []\n",
    "\n",
    "for n, article_url in enumerate(urls):\n",
    "    if 'wyborcza.pl' in article_url or 'gazeta.plt' in article_url:\n",
    "        driver.get(article_url)\n",
    "        try:\n",
    "            if 'wyborcza.pl' in article_url:\n",
    "                wyborcza_content.append(get_data(article_url))\n",
    "            else:\n",
    "                gazeta_content.append(get_data(article_url))\n",
    "        except:\n",
    "            bad_urls.append(article_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dir = 'data/articles/wyborcza/'\n",
    "if not exists(articles_dir):\n",
    "    makedirs(articles_dir)\n",
    "    \n",
    "with open(articles_dir+keyword, 'w') as f:\n",
    "    writer =  csv.writer(f)\n",
    "    writer.writerows(wyborcza_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dir = 'data/articles/gazeta/'\n",
    "if not exists(articles_dir):\n",
    "    makedirs(articles_dir)\n",
    "    \n",
    "with open(articles_dir+keyword, 'w') as f:\n",
    "    writer =  csv.writer(f)\n",
    "    writer.writerows(gazeta_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(url_dir+keyword+'_errors', 'a') as f:\n",
    "    f.write('\\n'.join(bad_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Options()\n",
    "opt.add_argument(\"--headless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = Chrome(options = opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('keywords', 'r') as f:\n",
    "    keywords = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pis', 'platforma', 'duda', 'morawiecki', 'szyd≈Ço']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Messy one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from os import makedirs\n",
    "from os.path import exists\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from time import sleep\n",
    "\n",
    "opt = Options()\n",
    "opt.add_argument(\"--headless\")\n",
    "\n",
    "with open('keywords', 'r') as f:\n",
    "    keywords = f.read().split('\\n')\n",
    "\n",
    "\n",
    "\n",
    "def make_dir(desired_dir):\n",
    "    if not exists(desired_dir):\n",
    "        makedirs(desired_dir)\n",
    "        \n",
    "def get_data(url):\n",
    "    if 'wyborcza.pl' in url:\n",
    "        title = driver.find_element_by_class_name('art-title').text\n",
    "        lead = driver.find_element_by_class_name('article-lead').text\n",
    "        content = driver.find_element_by_class_name('art_content').text\n",
    "        img_desc =  \"-@@@-\".join([x.text for x in driver.find_elements_by_class_name('article-image-desc')])\n",
    "        comments = \"-@@@-\".join([x.text for x in driver.find_elements_by_class_name('cBody') if x.text != ''])\n",
    "        return title, lead, content, img_desc, comments\n",
    "    elif 'gazeta.pl' in url:\n",
    "        title = driver.find_element_by_id('article_title').text\n",
    "        lead = driver.find_element_by_id('gazeta_article_lead').text\n",
    "        content = driver.find_element_by_class_name('art_content').text\n",
    "        img_desc =  \"-@@@-\".join([x.text for x in driver.find_elements_by_class_name('desc')])\n",
    "        comments = \"-@@@-\".join([x.find_elements_by_tag_name('p')[3].text for x in driver.find_elements_by_class_name('comment-body') if x.text != ''])\n",
    "        return title, lead, content, img_desc, comments\n",
    "\n",
    "    \n",
    "for keyword in keywords:\n",
    "    # get urls for that keyword using requests, for both wyborcza.pl and gazeta.pl\n",
    "    url_dir = 'data/wyborcza_gazeta/urls/'\n",
    "    urls = []\n",
    "    \n",
    "    for i in range(1, 2):\n",
    "        txt_html = requests\\\n",
    "            .get(\"http://szukaj.gazeta.pl/wyszukaj/artykul?&query=\"+keyword+\"&sortMode=SCORE&pageNumber=\"+str(i))\\\n",
    "            .text\n",
    "        soup = BeautifulSoup(txt_html)\n",
    "\n",
    "        elements = soup.find_all('section', 'elem')\n",
    "        if len(elements) != 0:\n",
    "            for elem in elements:\n",
    "                a = elem.header.h3.a\n",
    "                if a:\n",
    "                    urls.append(a['href'])\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    # save scrapped urls\n",
    "    make_dir(url_dir)\n",
    "    urls = list(set(urls))\n",
    "    with open(url_dir+keyword, 'w') as f:\n",
    "        f.write('\\n'.join(urls))\n",
    "    \n",
    "    # initialize selenium\n",
    "    driver = Chrome(options = opt)\n",
    "    driver.get(\"https://wyborcza.pl/0,0.html\")\n",
    "    try:\n",
    "        driver\\\n",
    "            .find_element_by_xpath('//*[@id=\"rodoNotificationWrapper\"]/div[2]/div/div[3]/button')\\\n",
    "            .click()\n",
    "    except:\n",
    "        pass\n",
    "    driver.find_element_by_id('wH_login_form').submit()\n",
    "    sleep(3)\n",
    "\n",
    "    driver.find_element_by_id('wyborczaEmail').send_keys('slusarczyk1@wp.pl')\n",
    "    driver.find_element_by_id('wyborczaPassword').send_keys('Sraniejebanko1')\n",
    "    sleep(1)\n",
    "    driver.find_element_by_xpath('/html/body/section/section[1]/form/div[4]/button').click()\n",
    "    sleep(3)\n",
    "    \n",
    "    # read backed up urls \n",
    "    with open(url_dir+keyword, 'r') as f:\n",
    "        urls = f.read().split('\\n')\n",
    "    \n",
    "    # get article content\n",
    "    bad_urls = []\n",
    "    wyborcza_content = []\n",
    "    gazeta_content = []\n",
    "\n",
    "    for article_url in urls:\n",
    "        if 'wyborcza.pl' in article_url or 'gazeta.plt' in article_url:\n",
    "            driver.get(article_url)\n",
    "            try:\n",
    "                if 'wyborcza.pl' in article_url:\n",
    "                    wyborcza_content.append(get_data(article_url))\n",
    "                else:\n",
    "                    gazeta_content.append(get_data(article_url))\n",
    "            except:\n",
    "                bad_urls.append(article_url)\n",
    "    \n",
    "    # save articles content for separately for wyborcza and gazeta\n",
    "    articles_dir = 'data/wyborcza/articles/'\n",
    "    make_dir(articles_dir)\n",
    "\n",
    "    with open(articles_dir+keyword, 'w') as f:\n",
    "        writer =  csv.writer(f)\n",
    "        writer.writerows(wyborcza_content)\n",
    "        \n",
    "    articles_dir = 'data/gazeta/articles/'\n",
    "    make_dir(articles_dir)\n",
    "\n",
    "    with open(articles_dir+keyword, 'w') as f:\n",
    "        writer =  csv.writer(f)\n",
    "        writer.writerows(gazeta_content)\n",
    "        \n",
    "    # and save urls script had problem with\n",
    "    with open(url_dir+keyword+'_errors', 'a') as f:\n",
    "        f.write('\\n'.join(bad_urls))\n",
    "        \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from os import makedirs, listdir\n",
    "from os.path import exists\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "# imp functions\n",
    "def make_dir(desired_dir):\n",
    "    if not exists(desired_dir):\n",
    "        makedirs(desired_dir)\n",
    "        \n",
    "def get_data(url):\n",
    "    if 'wyborcza.pl' in url:\n",
    "        try:\n",
    "            title = driver.find_element_by_xpath('//*[@id=\"art-header\"]/div[2]/h1').text\n",
    "        except:\n",
    "            try:\n",
    "                title = driver.find_element_by_class_name('art-title').text\n",
    "            except:\n",
    "                try:\n",
    "                    title = driver.find_element_by_xpath('/html/body/div[5]/div[1]/div/header/div[2]').text\n",
    "                except:\n",
    "                    try:\n",
    "                        title = driver.find_element_by_xpath('/html/body/div[4]/div[1]/div/header/div[2]/h1').text\n",
    "                    except:\n",
    "                        try:\n",
    "                            title = driver.find_element_by_xpath('/html/body/main/div[1]/div/header/div[2]/h1').text\n",
    "                        except:\n",
    "                            try:\n",
    "                                title = driver.find_element_by_xpath('/html/body/div[5]/div[1]/div/header/div[2]/h1').text\n",
    "                            except:\n",
    "                                try:\n",
    "                                    title = driver.find_element_by_xpath('/html/body/div[4]/div[1]/div/header/div[2]').text\n",
    "                                except:\n",
    "                                    title = None\n",
    "                    \n",
    "        lead = driver.find_element_by_class_name('article-lead').text\n",
    "        try:\n",
    "            content = driver.find_element_by_id('artykul').text\n",
    "        except:\n",
    "            content = driver.find_element_by_class_name('art_content').text\n",
    "        img_desc =  \"-@@@-\".join([x.text for x in driver.find_elements_by_class_name('article-image-desc')])\n",
    "        comments = \"-@@@-\".join([x.text for x in driver.find_elements_by_class_name('cBody') if x.text != ''])\n",
    "        return [title, lead, content, img_desc, comments]\n",
    "    elif 'gazeta.pl' in url:\n",
    "        title = driver.find_element_by_xpath('//*[@id=\"article_title\"]').text\n",
    "        lead = driver.find_element_by_id('gazeta_article_lead').text\n",
    "        try:\n",
    "            content = driver.find_element_by_id('artykul').text\n",
    "        except:\n",
    "            content = driver.find_element_by_class_name('art_content').text\n",
    "        img_desc =  \"-@@@-\".join([x.text for x in driver.find_elements_by_class_name('desc')])\n",
    "        comments = \"-@@@-\".join([x.find_elements_by_tag_name('p')[3].text for x in driver.find_elements_by_class_name('comment-body') if x.text != ''])\n",
    "        return [title, lead, content, img_desc, comments]\n",
    "\n",
    "\n",
    "# prepare urls\n",
    "url_dir = 'data/wyborcza_gazeta/urls/'\n",
    "with open('keywords', 'r') as f:\n",
    "    keywords = f.read().split('\\n')\n",
    "\n",
    "all_urls = []\n",
    "for collected_url_file in listdir(url_dir):\n",
    "    if 'ipynb' not in collected_url_file:\n",
    "        with open(url_dir+collected_url_file, 'r') as f:\n",
    "            tmp_urls = f.read().split('\\n')\n",
    "        for tmp_url in tmp_urls:\n",
    "            all_urls.append(tmp_url)\n",
    "all_urls = list(set(all_urls))\n",
    "print(len(all_urls))\n",
    "all_urls = [x for x in all_urls if x != '']\n",
    "all_urls = [x for x in all_urls if 'gazeta.pl' in x or 'wyborcza.pl' in x]\n",
    "print(len(all_urls))\n",
    "\n",
    "\n",
    "# initialize selenium\n",
    "opt = Options()\n",
    "opt.add_argument(\"--headless\")\n",
    "driver = Chrome(options = opt)\n",
    "# driver = Chrome()\n",
    "driver.get(\"https://wyborcza.pl/0,0.html\")\n",
    "try:\n",
    "    driver\\\n",
    "        .find_element_by_xpath('//*[@id=\"rodoNotificationWrapper\"]/div[2]/div/div[3]/button')\\\n",
    "        .click()\n",
    "except:\n",
    "    pass\n",
    "driver.find_element_by_id('wH_login_form').submit()\n",
    "\n",
    "driver.find_element_by_id('wyborczaEmail').send_keys('slusarczyk1@wp.pl')\n",
    "driver.find_element_by_id('wyborczaPassword').send_keys('Sraniejebanko1')\n",
    "driver.find_element_by_xpath('/html/body/section/section[1]/form/div[4]/button').click()\n",
    "sleep(1)\n",
    "\n",
    "\n",
    "# get article content\n",
    "bad_urls = []\n",
    "wyborcza_content = []\n",
    "gazeta_content = []\n",
    "i = 0\n",
    "for n, article_url in enumerate(all_urls[:300]):\n",
    "    # getpage\n",
    "    try:\n",
    "        driver.get(article_url)\n",
    "        if 'gazeta.pl' in article_url:\n",
    "            gazeta_content.append([article_url] + get_data(article_url))\n",
    "        elif 'wyborcza.pl' in article_url:\n",
    "            wyborcza_content.append([article_url] + get_data(article_url))\n",
    "        print(':)')\n",
    "        print(article_url)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(article_url)\n",
    "        bad_urls.append(article_url)\n",
    "    # every 100 pages save to new file good and bad ones\n",
    "    if n % 200 == 0:\n",
    "        i += 1\n",
    "        \n",
    "        articles_dir = 'data/gazeta/articles/'\n",
    "        make_dir(articles_dir)\n",
    "        with open(articles_dir + str(i), 'w') as f:\n",
    "            writer =  csv.writer(f)\n",
    "            writer.writerows(gazeta_content)\n",
    "            \n",
    "        articles_dir = 'data/wyborcza/articles/'\n",
    "        make_dir(articles_dir)\n",
    "        with open(articles_dir + str(i), 'w') as f:\n",
    "            writer =  csv.writer(f)\n",
    "            writer.writerows(wyborcza_content)\n",
    "    \n",
    "with open(url_dir+'errors', 'w') as f:\n",
    "    f.write('\\n'.join(bad_urls))\n",
    "    \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
